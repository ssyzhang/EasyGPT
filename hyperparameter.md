## 超参数设置

1. lr

125M 模型 6e-4
350M 模型 3e-4
760M 模型 2.5e-4

2. batch_size

 50万tokens
 对应480

3. 训练步数
训练 Tokens 数应该是参数量的 20 倍
303M*20=6.06B
6.06B/500K=12000步,但如今趋势是超量训练,可以为1.5万步